{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Notebook\n\n**Content:**\n- [Introduction to Machine Learning](#1)\n    - [EDA (Exploratory Data Analysis)](#2)\n    - [K-Nearest Neighbors (KNN)](#3)\n    - [Regression](#4)\n    - [Random Forest](#5)\n    - [Model Evaluation](#6)\n\n  \n  ","metadata":{"_uuid":"819c6b57207e720308ea29491d85a6e8af8c253e","_cell_guid":"dcc6c619-c02a-4bea-b322-4f32779ae672"}},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n### INTRODUCTION TO MACHINE LEARNING\n\nThis is a simple demo of Machine Learning codes for Malaysia ML Study Jam. This notebook is adopted from [here](https://www.kaggle.com/code/kanncaa1/machine-learning-tutorial-for-beginners/notebook).","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"2b90d6250c8f9c2c302c849bffa132bd3483e893","_cell_guid":"5ee3a7aa-eca4-411b-9f84-d14c09e13730","execution":{"iopub.status.busy":"2022-08-12T03:44:45.218510Z","iopub.execute_input":"2022-08-12T03:44:45.218834Z","iopub.status.idle":"2022-08-12T03:44:45.247029Z","shell.execute_reply.started":"2022-08-12T03:44:45.218787Z","shell.execute_reply":"2022-08-12T03:44:45.245926Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# read csv (comma separated value) into data\ndata = pd.read_csv('../input/biomechanical/column_2C.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","metadata":{"_uuid":"a9c5426e9e5cef81c1e1639ebe57e9b45dfd2c43","_cell_guid":"32af03f6-41be-41ec-9023-8cd519040984","execution":{"iopub.status.busy":"2022-08-12T03:44:45.248592Z","iopub.execute_input":"2022-08-12T03:44:45.248912Z","iopub.status.idle":"2022-08-12T03:44:45.268924Z","shell.execute_reply.started":"2022-08-12T03:44:45.248851Z","shell.execute_reply":"2022-08-12T03:44:45.268138Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n### EXPLORATORY DATA ANALYSIS (EDA)\n* In order to make something in data, we will explore the data. \n* Always start with *head()* to see features that are *pelvic_incidence,\tpelvic_tilt numeric,\tlumbar_lordosis_angle,\tsacral_slope,\tpelvic_radius* and \t*degree_spondylolisthesis* and target variable that is *class*\n* head(): default value of it shows first 5 rows(samples). If you want to see for example 100 rows just write head(100)\n","metadata":{"_uuid":"a0e671bf2ef8dbe81da2705ad70b69401bb7af16","_cell_guid":"65e897a1-8259-44c5-9cb7-e5e653f9032d"}},{"cell_type":"code","source":"# to see features and target variable\ndata.head()","metadata":{"_uuid":"9a5993f4962882e1156f2062b7abf706a0739d51","_cell_guid":"c1ecd622-67cc-485f-bfa7-8c682d30a5eb","execution":{"iopub.status.busy":"2022-08-12T03:44:45.270174Z","iopub.execute_input":"2022-08-12T03:44:45.270533Z","iopub.status.idle":"2022-08-12T03:44:45.305782Z","shell.execute_reply.started":"2022-08-12T03:44:45.270477Z","shell.execute_reply":"2022-08-12T03:44:45.305113Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","metadata":{"_uuid":"b7b9addc824de1a35b67d96d3092ffcb10869947","_cell_guid":"1631690c-bb9d-4460-a7d9-a335aa914b4f","execution":{"iopub.status.busy":"2022-08-12T03:44:45.306966Z","iopub.execute_input":"2022-08-12T03:44:45.307456Z","iopub.status.idle":"2022-08-12T03:44:45.316618Z","shell.execute_reply.started":"2022-08-12T03:44:45.307408Z","shell.execute_reply":"2022-08-12T03:44:45.315555Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"As you can see:\n* length: 310 (range index)\n* Features are float\n* Target variables are object that is like string\n* Now we have some ideas about data but lets look go inside data deeper\n    * Why we need to see statistics like mean, std, max or min? Answer: In order to visualize data, values should be closer each other. As you can see values looks like closer. At least there is no incompatible values like mean of one feature is 0.1 and other is 1000. For some machine learning models, you may need to scale and normalise the data.","metadata":{"_uuid":"96f97c33305956eb76d8a2043fd71aff05e38548","_cell_guid":"a7dd2a6f-a81d-4dce-9d74-fd0148c446ae"}},{"cell_type":"code","source":"data.describe()","metadata":{"_uuid":"69d132068cce6a915aac7678b4e9fbcf3e365643","_cell_guid":"137897ca-b519-4ac3-afdd-6c4136447e39","execution":{"iopub.status.busy":"2022-08-12T03:44:45.317928Z","iopub.execute_input":"2022-08-12T03:44:45.318265Z","iopub.status.idle":"2022-08-12T03:44:45.358308Z","shell.execute_reply.started":"2022-08-12T03:44:45.318208Z","shell.execute_reply":"2022-08-12T03:44:45.357517Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"pd.plotting.scatter_matrix:\n* green: *normal* and red: *abnormal*\n* c:  color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type ","metadata":{"_uuid":"c8961b1f3a3a73547a0b7d27955f9844f6ad43eb","_cell_guid":"3776dd3d-d0aa-419e-b788-e75454e94b86"}},{"cell_type":"code","source":"color_list = ['red' if i=='Abnormal' else 'Cyan' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()\n","metadata":{"_uuid":"5dc0763dde8b2638a5289f0b4496f384f85aca85","_cell_guid":"fb106765-bb47-452b-8d6e-3578b479873c","execution":{"iopub.status.busy":"2022-08-12T03:44:45.359645Z","iopub.execute_input":"2022-08-12T03:44:45.359993Z","iopub.status.idle":"2022-08-12T03:44:48.955200Z","shell.execute_reply.started":"2022-08-12T03:44:45.359923Z","shell.execute_reply":"2022-08-12T03:44:48.954311Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Okay, as you understand in scatter matrix there are relations between each feature but how many *normal(green)* and *abnormal(red)* classes are there. \n* Searborn library has *countplot()* that counts number of classes\n* Also you can print it with *value_counts()* method\n\n<br> This data looks like balanced. Actually there is no definiton or numeric value of balanced data but this data is balanced enough for us.\n<br> Now lets learn first classification method KNN","metadata":{"_uuid":"d77cef37b8c5c4f07d3f4aa94cc4ad1ccbd7caca","_cell_guid":"53fc7ab6-de8b-4b8f-9e4a-38c5db72eea0"}},{"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","metadata":{"_uuid":"e1bb9fd338e6900888e2e4717f54be46cee848a2","_cell_guid":"36243fa5-1fa6-4f8b-bc16-43449b0dc898","execution":{"iopub.status.busy":"2022-08-12T03:44:48.956652Z","iopub.execute_input":"2022-08-12T03:44:48.957046Z","iopub.status.idle":"2022-08-12T03:44:49.115669Z","shell.execute_reply.started":"2022-08-12T03:44:48.956986Z","shell.execute_reply":"2022-08-12T03:44:49.114744Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n###  K-NEAREST NEIGHBORS (KNN)\n* KNN: Look at the K closest labeled data points\n* Classification method.\n* First we need to train our data. Train = fit\n* fit(): fits the data, train the data.\n* predict(): predicts the data\n<br> If you do not understand what is KNN, look at youtube there are videos like 4-5 minutes. You can understand better with it.\n<br> Lets learn how to implement it with sklearn\n* x: features\n* y: target variables(normal, abnormal)\n* n_neighbors: K. In this example it is 3. it means that Look at the 3 closest labeled data points\n","metadata":{"_uuid":"9263c479815bb729dad40bf01b68aa18a3c946ac","_cell_guid":"24a5d90f-3e7d-4733-a6f9-ff4f51145155"}},{"cell_type":"code","source":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","metadata":{"_uuid":"854f0a3898a928640b9714fcd584e48c9b377f9f","_cell_guid":"c717491d-2bd5-4dc7-ac13-b9f581b1cddd","execution":{"iopub.status.busy":"2022-08-12T03:44:49.117212Z","iopub.execute_input":"2022-08-12T03:44:49.117847Z","iopub.status.idle":"2022-08-12T03:44:49.137443Z","shell.execute_reply.started":"2022-08-12T03:44:49.117783Z","shell.execute_reply":"2022-08-12T03:44:49.136508Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"* Well, we fit the data and predict it with KNN. \n* So, do we predict correct or what is our accuracy or the accuracy is best metric to evaluate our result? Lets give answer of this questions\n<br> Measuring model performance:\n* Accuracy which is fraction of correct predictions is commonly used metric. We will use it know but there is another problem\n\n<br>As you see I train data with x (features) and again predict the x(features). Yes you are reading right but yes you are right again it is absurd :)\n\n<br>Therefore we need to split our data train and test sets.\n* train: use train set by fitting\n* test: make prediction on test set.\n* With train and test sets, fitted data and tested data are completely different\n* train_test_split(x,y,test_size = 0.3,random_state = 1)\n    * x: features\n    * y: target variables (normal,abnormal)\n    * test_size: percentage of test size. Example test_size = 0.3, test size = 30% and train size = 70%\n    * random_state: sets a seed. If this seed is same number, train_test_split() produce exact same split at each time\n* fit(x_train,y_train): fit on train sets\n* score(x_test,y_test)): predict and give accuracy on test sets","metadata":{"_uuid":"6d9c3eacd279ddf7c0ef33b9e1814cd549d0feaa","_cell_guid":"b5d85f4e-ab30-4c49-b2bc-6265b6baea9d"}},{"cell_type":"markdown","source":"### Training, Validation, Testing Dataset\n\n![train-test](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgLv631haYO6IDGY5NkE0HuWBsZGYaxzktvtgPkc524iyIwdL68hMKtiHLbbw-syezxaXXRln7LK1FB0S0uMsQxvoC_izakx5JloLlNHeTaHBXxx9J_MNFKKDpqcV_9GqhfdzXlsPLm7OgbjD1ThTYYcWYz01O3AhKo_spqobTmlGTZcJSMNrc6Rv20/s1600/Untitled.pn)","metadata":{}},{"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","metadata":{"_uuid":"4702429fdfa62650937b09fde5a8fd3136da8c55","_cell_guid":"79865658-c89f-43c8-a1a9-75acc4feab6a","execution":{"iopub.status.busy":"2022-08-12T03:44:49.139004Z","iopub.execute_input":"2022-08-12T03:44:49.139654Z","iopub.status.idle":"2022-08-12T03:44:49.166440Z","shell.execute_reply.started":"2022-08-12T03:44:49.139590Z","shell.execute_reply":"2022-08-12T03:44:49.165510Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Accuracy is 86% so is it good ? We need to find out more or perform model evaluations.\n<br> Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\n<br> Model complexity:\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace. \n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit. \n* At below, I range K value from 1 to 25(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memozize train sets and cannot give good accuracy on test set (overfit). Also if K is 18, model is lead to underfit. Again accuracy is not enough. However look at when K is 18(best performance), accuracy has highest value almost 88%. \n\n","metadata":{"_uuid":"544f51ef05efe0b3ae4b02da806778bcfa715f35","_cell_guid":"a5665258-3f7f-435a-a634-49eb0c0d66e0"}},{"cell_type":"code","source":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","metadata":{"_uuid":"18d8739373085a9964071f38b8f2adcb64f25491","_cell_guid":"db2c7062-ce1b-4e8b-9b2f-0ee0cd679a91","execution":{"iopub.status.busy":"2022-08-12T03:44:49.170402Z","iopub.execute_input":"2022-08-12T03:44:49.170772Z","iopub.status.idle":"2022-08-12T03:44:50.022798Z","shell.execute_reply.started":"2022-08-12T03:44:49.170703Z","shell.execute_reply":"2022-08-12T03:44:50.021988Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"### Up to this point what you learn:\n* Supervised learning\n* Exploratory data analysis\n* KNN\n    * How to split data\n    * How to fit, predict data\n    * How to measure medel performance (accuracy)\n    * How to choose hyperparameter (K)\n    \n**<br> What happens if I chance the title KNN and make it some other classification technique like Random Forest?**\n* The answer is **nothing**. What you need to is just watch a video about what is random forest in youtube and implement what you learn in KNN. Because the idea and even most of the codes (only KNeighborsClassifier need to be RandomForestClassifier ) are same. You need to split, fit, predict your data and measue performance and choose hyperparameter of random forest(like max_depth). ","metadata":{"_uuid":"96423b4f710966c8071647874623c139c1c79bb7","_cell_guid":"b598ee81-e535-49c0-b53c-b13b0a5058db"}},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n### REGRESSION\n* Supervised learning\n* We will learn linear regression\n* This orthopedic patients data is not proper for regression so we will only use two features that are *sacral_slope* and *pelvic_incidence* of abnormal \n    * Consider only feature is pelvic_incidence and target is sacral_slope \n    * Lets look at scatter plot so as to understand it better\n    * reshape(-1,1): If you do not use it shape of x or y becaomes (210,) and we cannot use it in sklearn, so we use shape(-1,1) and shape of x or y be (210, 1). ","metadata":{"_uuid":"d075fd2a7c05e5414e33b7b1314d81a6b945e7b3","_cell_guid":"f9d427a9-faa5-46cf-9e3c-2c8cea2571ad"}},{"cell_type":"code","source":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = data[data['class'] =='Abnormal']\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","metadata":{"_uuid":"d2655c140423b1228c42d2e8dfe54344ba43dcb0","_cell_guid":"6b072c42-059f-4e45-9cfa-8ed39b274f72","execution":{"iopub.status.busy":"2022-08-12T03:44:50.023907Z","iopub.execute_input":"2022-08-12T03:44:50.024289Z","iopub.status.idle":"2022-08-12T03:44:50.284258Z","shell.execute_reply.started":"2022-08-12T03:44:50.024239Z","shell.execute_reply":"2022-08-12T03:44:50.283313Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Now we have our data to make regression. In regression problems target value is continuously varying variable such as price of house or sacral_slope. Lets fit line into this points.\n\n<br> Linear regression\n* y = ax + b       where  y = target, x = feature and a = parameter of model\n* We choose parameter of model(a) according to minimum error function that is lost function\n* In linear regression we use Ordinary Least Square (OLS) as lost function.\n* OLS: sum all residuals but some positive and negative residuals can cancel each other so we sum of square of residuals. It is called OLS\n* Score: Score uses R^2 method that is ((y_pred - y_mean)^2 )/(y_actual - y_mean)^2","metadata":{"_uuid":"c84719d363ff736e96a0a575dfd0381bcbc549fb","_cell_guid":"874ac5e0-5bc0-4429-b6c5-707690b5dd77"}},{"cell_type":"code","source":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","metadata":{"_uuid":"7cdc74efa8c46dab5f14f6cc2779928c11a4fa62","_cell_guid":"fb7991f3-5869-4df0-bf6c-30f61e8215c6","execution":{"iopub.status.busy":"2022-08-12T03:44:50.285718Z","iopub.execute_input":"2022-08-12T03:44:50.286249Z","iopub.status.idle":"2022-08-12T03:44:50.489235Z","shell.execute_reply.started":"2022-08-12T03:44:50.286186Z","shell.execute_reply":"2022-08-12T03:44:50.488273Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n### RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"# Confusion matrix with random forest\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nrf = RandomForestClassifier(random_state = 4)\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T03:44:50.490701Z","iopub.execute_input":"2022-08-12T03:44:50.491232Z","iopub.status.idle":"2022-08-12T03:44:50.539751Z","shell.execute_reply.started":"2022-08-12T03:44:50.491169Z","shell.execute_reply":"2022-08-12T03:44:50.538804Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","metadata":{"_uuid":"f2697bbc248102687596713406512b4cb7f24929","_cell_guid":"fcba81bb-1257-48d0-afe1-a4416237cc73","execution":{"iopub.status.busy":"2022-08-12T03:44:50.541052Z","iopub.execute_input":"2022-08-12T03:44:50.541586Z","iopub.status.idle":"2022-08-12T03:44:50.688131Z","shell.execute_reply.started":"2022-08-12T03:44:50.541526Z","shell.execute_reply":"2022-08-12T03:44:50.687245Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### MODEL EVALUATION\nNow lets discuss accuracy. Is it enough for measurement of model selection. For example, there is a data that includes 95% normal and 5% abnormal samples and our model uses accuracy for measurement metric. Then our model predict 100% normal for all samples and accuracy is 95% but it classify all abnormal samples wrong. Therefore we need to use confusion matrix as a model measurement matris in imbalance data.\nWhile using confusion matrix lets use Random forest classifier to diversify classification methods.\n\n![confusion_matrix](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEipcXJkfwq-zXlQ14SKgEUkfiy1kktjpnGnMP9SjbSXJA1Qu21IV04dYJDeA5zbjqIsLIbbPrP_6fGdXJh-SMFx2ktCxz1vlC_Pa3WaAFjcSrexYVRn1_Cgn3CX2oFcwJ3VmfiKZt37f7ajiVD3qPIdKmakwkn2vGJImv_kM4O1DbbOmvXZ9b1JI4br/s600/cm.png)\n\n- tp = true positive (20), fp = false positive (7), fn = false negative (8), tn = true negative (58)\n- tp = Prediction is positive (normal) and actual is positive (normal).\n- fp = Prediction is positive (normal) and actual is negative (abnormal).\n- fn = Prediction is negative (abnormal) and actual is positive (normal).\n- tn = Prediction is negative (abnormal) and actual is negative (abnormal)\n- precision = tp / (tp+fp)\n- recall = tp / (tp+fn)\n- f1 = 2 precision recall / ( precision + recall)","metadata":{"_uuid":"dbe175099c0c8151c16ad0c78f1414de8fd9ebdc","_cell_guid":"4552b965-e15d-4e05-9e02-224159e8d508"}},{"cell_type":"markdown","source":"# CONCLUSION\nThis is the end of the ML Study Jam demo. Please go through the material in [Kaggle Learn](http://www.kaggle.com/learn) to learn more.","metadata":{"_uuid":"2c1a42c1de45ce53e04761b4b6c05840fddaee95","_cell_guid":"41762172-8d1f-47ef-b8ab-79fb3a2b0a19"}}]}
